{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFOs and preprocessing\n",
    "#### Checking column types\n",
    "Take a look at the UFO dataset's column types using the .info() method. Two columns jump out for transformation: the seconds column, which is a numeric column but is being read in as object, and the date column, which can be transformed into the datetime type. That will make our feature engineering efforts easier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>type</th>\n",
       "      <th>seconds</th>\n",
       "      <th>length_of_time</th>\n",
       "      <th>desc</th>\n",
       "      <th>recorded</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/3/2011 19:21</td>\n",
       "      <td>woodville</td>\n",
       "      <td>wi</td>\n",
       "      <td>us</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1209600.0</td>\n",
       "      <td>2 weeks</td>\n",
       "      <td>Red blinking objects similar to airplanes or s...</td>\n",
       "      <td>12/12/2011</td>\n",
       "      <td>44.9530556</td>\n",
       "      <td>-92.291111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/3/2004 19:05</td>\n",
       "      <td>cleveland</td>\n",
       "      <td>oh</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30sec.</td>\n",
       "      <td>Many fighter jets flying towards UFO</td>\n",
       "      <td>10/27/2004</td>\n",
       "      <td>41.4994444</td>\n",
       "      <td>-81.695556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/25/2009 21:00</td>\n",
       "      <td>coon rapids</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>cigar</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green&amp;#44 red&amp;#44 and blue pulses of light tha...</td>\n",
       "      <td>12/12/2009</td>\n",
       "      <td>45.1200000</td>\n",
       "      <td>-93.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/21/2002 05:45</td>\n",
       "      <td>clemmons</td>\n",
       "      <td>nc</td>\n",
       "      <td>us</td>\n",
       "      <td>triangle</td>\n",
       "      <td>300.0</td>\n",
       "      <td>about 5 minutes</td>\n",
       "      <td>It was a large&amp;#44 triangular shaped flying ob...</td>\n",
       "      <td>12/23/2002</td>\n",
       "      <td>36.0213889</td>\n",
       "      <td>-80.382222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/19/2010 12:55</td>\n",
       "      <td>calgary (canada)</td>\n",
       "      <td>ab</td>\n",
       "      <td>ca</td>\n",
       "      <td>oval</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>A white spinning disc in the shape of an oval.</td>\n",
       "      <td>8/24/2010</td>\n",
       "      <td>51.083333</td>\n",
       "      <td>-114.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>7/5/2000 19:30</td>\n",
       "      <td>schnecksville</td>\n",
       "      <td>pa</td>\n",
       "      <td>us</td>\n",
       "      <td>oval</td>\n",
       "      <td>5.0</td>\n",
       "      <td>about 5 seconds</td>\n",
       "      <td>On my bike when i saw a shiny silver oval not ...</td>\n",
       "      <td>7/11/2000</td>\n",
       "      <td>40.6677778</td>\n",
       "      <td>-75.607500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>3/18/2008 22:00</td>\n",
       "      <td>gibson</td>\n",
       "      <td>ga</td>\n",
       "      <td>us</td>\n",
       "      <td>triangle</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25 seconds</td>\n",
       "      <td>Three sided  stationary object turning clockwi...</td>\n",
       "      <td>3/31/2008</td>\n",
       "      <td>33.2333333</td>\n",
       "      <td>-82.595556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4932</th>\n",
       "      <td>6/15/2005 02:30</td>\n",
       "      <td>kent</td>\n",
       "      <td>wa</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>0.0</td>\n",
       "      <td>early morning</td>\n",
       "      <td>Cicle object over Washington state all differe...</td>\n",
       "      <td>10/30/2006</td>\n",
       "      <td>47.3811111</td>\n",
       "      <td>-122.233611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>11/1/1991 03:00</td>\n",
       "      <td>niles</td>\n",
       "      <td>mi</td>\n",
       "      <td>us</td>\n",
       "      <td>triangle</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>2 hours</td>\n",
       "      <td>Triangle zigzagged.  Another shined light on u...</td>\n",
       "      <td>9/2/2005</td>\n",
       "      <td>41.8297222</td>\n",
       "      <td>-86.254167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4934</th>\n",
       "      <td>12/10/2005 18:00</td>\n",
       "      <td>phoenix</td>\n",
       "      <td>az</td>\n",
       "      <td>us</td>\n",
       "      <td>other</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1 minutes</td>\n",
       "      <td>Close encounter of the third kind.</td>\n",
       "      <td>12/20/2012</td>\n",
       "      <td>33.4483333</td>\n",
       "      <td>-112.073333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4935 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date              city state country      type    seconds  \\\n",
       "0      11/3/2011 19:21         woodville    wi      us   unknown  1209600.0   \n",
       "1      10/3/2004 19:05         cleveland    oh      us    circle       30.0   \n",
       "2      9/25/2009 21:00       coon rapids    mn      us     cigar        0.0   \n",
       "3     11/21/2002 05:45          clemmons    nc      us  triangle      300.0   \n",
       "4      8/19/2010 12:55  calgary (canada)    ab      ca      oval        0.0   \n",
       "...                ...               ...   ...     ...       ...        ...   \n",
       "4930    7/5/2000 19:30     schnecksville    pa      us      oval        5.0   \n",
       "4931   3/18/2008 22:00            gibson    ga      us  triangle       25.0   \n",
       "4932   6/15/2005 02:30              kent    wa      us    circle        0.0   \n",
       "4933   11/1/1991 03:00             niles    mi      us  triangle     7200.0   \n",
       "4934  12/10/2005 18:00           phoenix    az      us     other       60.0   \n",
       "\n",
       "       length_of_time                                               desc  \\\n",
       "0             2 weeks  Red blinking objects similar to airplanes or s...   \n",
       "1              30sec.               Many fighter jets flying towards UFO   \n",
       "2                 NaN  Green&#44 red&#44 and blue pulses of light tha...   \n",
       "3     about 5 minutes  It was a large&#44 triangular shaped flying ob...   \n",
       "4                   2     A white spinning disc in the shape of an oval.   \n",
       "...               ...                                                ...   \n",
       "4930  about 5 seconds  On my bike when i saw a shiny silver oval not ...   \n",
       "4931       25 seconds  Three sided  stationary object turning clockwi...   \n",
       "4932    early morning  Cicle object over Washington state all differe...   \n",
       "4933          2 hours  Triangle zigzagged.  Another shined light on u...   \n",
       "4934        1 minutes                 Close encounter of the third kind.   \n",
       "\n",
       "        recorded         lat        long  \n",
       "0     12/12/2011  44.9530556  -92.291111  \n",
       "1     10/27/2004  41.4994444  -81.695556  \n",
       "2     12/12/2009  45.1200000  -93.287500  \n",
       "3     12/23/2002  36.0213889  -80.382222  \n",
       "4      8/24/2010   51.083333 -114.083333  \n",
       "...          ...         ...         ...  \n",
       "4930   7/11/2000  40.6677778  -75.607500  \n",
       "4931   3/31/2008  33.2333333  -82.595556  \n",
       "4932  10/30/2006  47.3811111 -122.233611  \n",
       "4933    9/2/2005  41.8297222  -86.254167  \n",
       "4934  12/20/2012  33.4483333 -112.073333  \n",
       "\n",
       "[4935 rows x 11 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ufo = pd.read_csv('ufo_sightings_large.TXT')\n",
    "ufo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4935 entries, 0 to 4934\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   date            4935 non-null   object \n",
      " 1   city            4926 non-null   object \n",
      " 2   state           4516 non-null   object \n",
      " 3   country         4255 non-null   object \n",
      " 4   type            4776 non-null   object \n",
      " 5   seconds         4935 non-null   float64\n",
      " 6   length_of_time  4792 non-null   object \n",
      " 7   desc            4932 non-null   object \n",
      " 8   recorded        4935 non-null   object \n",
      " 9   lat             4935 non-null   object \n",
      " 10  long            4935 non-null   float64\n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 424.2+ KB\n"
     ]
    }
   ],
   "source": [
    "ufo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4935 entries, 0 to 4934\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   date            4935 non-null   datetime64[ns]\n",
      " 1   city            4926 non-null   object        \n",
      " 2   state           4516 non-null   object        \n",
      " 3   country         4255 non-null   object        \n",
      " 4   type            4776 non-null   object        \n",
      " 5   seconds         4935 non-null   float64       \n",
      " 6   length_of_time  4792 non-null   object        \n",
      " 7   desc            4932 non-null   object        \n",
      " 8   recorded        4935 non-null   object        \n",
      " 9   lat             4935 non-null   object        \n",
      " 10  long            4935 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(2), object(8)\n",
      "memory usage: 424.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Change the type of seconds to float\n",
    "ufo[\"seconds\"] = ufo['seconds'].astype('float')\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo[\"date\"] = pd.to_datetime(ufo['date'])\n",
    "\n",
    "# Check the column types\n",
    "print(ufo.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice job on transforming the column types! This will make feature engineering and standardization much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping missing data\n",
    "In this exercise, you'll remove some of the rows where certain columns have missing values. You're going to look at the length_of_time column, the state column, and the type column. You'll drop any row that contains a missing value in at least one of these three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4935, 3) \n",
      "\n",
      "length_of_time    143\n",
      "state             419\n",
      "type              159\n",
      "dtype: int64\n",
      "(4283, 11)\n"
     ]
    }
   ],
   "source": [
    "print(ufo[['length_of_time', 'state', 'type']].shape, \"\\n\")\n",
    "\n",
    "# Count the missing values in the length_of_time, state, and type columns, in that order\n",
    "print(ufo[['length_of_time', 'state', 'type']].isna().sum())\n",
    "\n",
    "# Drop rows where length_of_time, state, or type are missing\n",
    "ufo_no_missing = ufo.dropna(subset = ['length_of_time', 'state', 'type'])\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "print(ufo_no_missing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We'll work with this set going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables and standardization\n",
    "#### Extracting numbers from strings\n",
    "The length_of_time field in the UFO dataset is a text field that has the number of minutes within the string. Here, you'll extract that number from that text field using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    length_of_time  minutes\n",
      "0          2 weeks      2.0\n",
      "1           30sec.     30.0\n",
      "2              nan      NaN\n",
      "3  about 5 minutes      5.0\n",
      "4                2      2.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "ufo['length_of_time'] = ufo['length_of_time'].astype(str) # We wrote that code for 'TypeError: expected string or bytes-like object \n",
    "                                             # ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)'\n",
    "def return_minutes(time_string):\n",
    "\n",
    "    # Search for numbers in time_string\n",
    "    num = re.search('\\d+', time_string)\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "        \n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "print(ufo[[\"length_of_time\", \"minutes\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice job! The minutes information is now in a form where it can be inputted into a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying features for standardization\n",
    "In this exercise, you'll investigate the variance of columns in the UFO dataset to determine which features should be standardized. After taking a look at the variances of the seconds and minutes column, you'll see that the variance of the seconds column is extremely high. Because seconds and minutes are related to each other (an issue we'll deal with when we select features for modeling), let's log normalize the seconds column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds    3.156735e+10\n",
      "minutes    8.425929e+02\n",
      "dtype: float64\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:679: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check the variance of the seconds and minutes columns\n",
    "print(ufo[['seconds','minutes']].var())\n",
    "\n",
    "# Log normalize the seconds column\n",
    "ufo[\"seconds_log\"] = np.log(ufo[\"seconds\"])\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo[\"seconds_log\"].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good work! Now it's time to engineer new features in the ufo dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering new features\n",
    "#### Encoding categorical variables\n",
    "There are couple of columns in the UFO dataset that need to be encoded before they can be modeled through scikit-learn. You'll do that transformation here, using both binary and one-hot encoding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       0\n",
       "       ..\n",
       "4930    1\n",
       "4931    1\n",
       "4932    1\n",
       "4933    1\n",
       "4934    1\n",
       "Name: country_enc, Length: 4935, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas to encode us values as 1 and others as 0\n",
    "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda a: 1 if a == 'us' else 0)\n",
    "ufo[\"country_enc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['us', 'ca', nan, 'au', 'gb', 'de'], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufo[\"country\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['us' 'ca' nan 'au' 'gb' 'de']\n",
      "22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>type</th>\n",
       "      <th>seconds</th>\n",
       "      <th>length_of_time</th>\n",
       "      <th>desc</th>\n",
       "      <th>recorded</th>\n",
       "      <th>lat</th>\n",
       "      <th>...</th>\n",
       "      <th>flash</th>\n",
       "      <th>formation</th>\n",
       "      <th>light</th>\n",
       "      <th>other</th>\n",
       "      <th>oval</th>\n",
       "      <th>rectangle</th>\n",
       "      <th>sphere</th>\n",
       "      <th>teardrop</th>\n",
       "      <th>triangle</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-11-03 19:21:00</td>\n",
       "      <td>woodville</td>\n",
       "      <td>wi</td>\n",
       "      <td>us</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1209600.0</td>\n",
       "      <td>2 weeks</td>\n",
       "      <td>Red blinking objects similar to airplanes or s...</td>\n",
       "      <td>12/12/2011</td>\n",
       "      <td>44.9530556</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-10-03 19:05:00</td>\n",
       "      <td>cleveland</td>\n",
       "      <td>oh</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30sec.</td>\n",
       "      <td>Many fighter jets flying towards UFO</td>\n",
       "      <td>10/27/2004</td>\n",
       "      <td>41.4994444</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-09-25 21:00:00</td>\n",
       "      <td>coon rapids</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>cigar</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>Green&amp;#44 red&amp;#44 and blue pulses of light tha...</td>\n",
       "      <td>12/12/2009</td>\n",
       "      <td>45.1200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-11-21 05:45:00</td>\n",
       "      <td>clemmons</td>\n",
       "      <td>nc</td>\n",
       "      <td>us</td>\n",
       "      <td>triangle</td>\n",
       "      <td>300.0</td>\n",
       "      <td>about 5 minutes</td>\n",
       "      <td>It was a large&amp;#44 triangular shaped flying ob...</td>\n",
       "      <td>12/23/2002</td>\n",
       "      <td>36.0213889</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-08-19 12:55:00</td>\n",
       "      <td>calgary (canada)</td>\n",
       "      <td>ab</td>\n",
       "      <td>ca</td>\n",
       "      <td>oval</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>A white spinning disc in the shape of an oval.</td>\n",
       "      <td>8/24/2010</td>\n",
       "      <td>51.083333</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date              city state country      type    seconds  \\\n",
       "0 2011-11-03 19:21:00         woodville    wi      us   unknown  1209600.0   \n",
       "1 2004-10-03 19:05:00         cleveland    oh      us    circle       30.0   \n",
       "2 2009-09-25 21:00:00       coon rapids    mn      us     cigar        0.0   \n",
       "3 2002-11-21 05:45:00          clemmons    nc      us  triangle      300.0   \n",
       "4 2010-08-19 12:55:00  calgary (canada)    ab      ca      oval        0.0   \n",
       "\n",
       "    length_of_time                                               desc  \\\n",
       "0          2 weeks  Red blinking objects similar to airplanes or s...   \n",
       "1           30sec.               Many fighter jets flying towards UFO   \n",
       "2              nan  Green&#44 red&#44 and blue pulses of light tha...   \n",
       "3  about 5 minutes  It was a large&#44 triangular shaped flying ob...   \n",
       "4                2     A white spinning disc in the shape of an oval.   \n",
       "\n",
       "     recorded         lat  ...  flash  formation  light  other  oval  \\\n",
       "0  12/12/2011  44.9530556  ...      0          0      0      0     0   \n",
       "1  10/27/2004  41.4994444  ...      0          0      0      0     0   \n",
       "2  12/12/2009  45.1200000  ...      0          0      0      0     0   \n",
       "3  12/23/2002  36.0213889  ...      0          0      0      0     0   \n",
       "4   8/24/2010   51.083333  ...      0          0      0      0     1   \n",
       "\n",
       "   rectangle  sphere  teardrop  triangle  unknown  \n",
       "0          0       0         0         0        1  \n",
       "1          0       0         0         0        0  \n",
       "2          0       0         0         0        0  \n",
       "3          0       0         0         1        0  \n",
       "4          0       0         0         0        0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ufo[\"country\"].unique())\n",
    "\n",
    "# Use pandas to encode us values as 1 and others as 0\n",
    "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda a: 1 if a == 'us' else 0 )\n",
    "\n",
    "# Print the number of unique type values\n",
    "print(len(ufo['type'].unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo['type'])\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)\n",
    "\n",
    "ufo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome work! Let's continue on by extracting date components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features from dates\n",
    "Another feature engineering task to perform is month and year extraction. Perform this task on the date column of the ufo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2011-11-03 19:21:00\n",
      "1   2004-10-03 19:05:00\n",
      "2   2009-09-25 21:00:00\n",
      "3   2002-11-21 05:45:00\n",
      "4   2010-08-19 12:55:00\n",
      "Name: date, dtype: datetime64[ns] \n",
      "\n",
      "\n",
      "                 date  month  year\n",
      "0 2011-11-03 19:21:00     11  2011\n",
      "1 2004-10-03 19:05:00     10  2004\n",
      "2 2009-09-25 21:00:00      9  2009\n",
      "3 2002-11-21 05:45:00     11  2002\n",
      "4 2010-08-19 12:55:00      8  2010\n"
     ]
    }
   ],
   "source": [
    "# Look at the first 5 rows of the date column\n",
    "print(ufo['date'].head(),\"\\n\\n\")\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo[\"month\"] = ufo[\"date\"].dt.month\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo[\"year\"] = ufo[\"date\"].dt.year\n",
    "\n",
    "# Take a look at the head of all three columns\n",
    "print(ufo[[\"date\",\"month\",\"year\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice job on extracting dates! The pandas series attributes .dt.month and .dt.year are extremely useful for extraction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text vectorization\n",
    "You'll now transform the desc column in the UFO dataset into tf/idf vectors, since there's likely something we can learn from this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Red blinking objects similar to airplanes or s...\n",
      "1                 Many fighter jets flying towards UFO\n",
      "2    Green&#44 red&#44 and blue pulses of light tha...\n",
      "3    It was a large&#44 triangular shaped flying ob...\n",
      "4       A white spinning disc in the shape of an oval.\n",
      "Name: desc, dtype: object \n",
      "\n",
      "\n",
      "  (0, 599)\t0.3196929518112806\n",
      "  (0, 6204)\t0.32725095835678897\n",
      "  (0, 5937)\t0.16053126945167856\n",
      "  (0, 5223)\t0.10691429469459467\n",
      "  (0, 3091)\t0.08229067463398756\n",
      "  (0, 757)\t0.1948218553551395\n",
      "  (0, 3964)\t0.3652351192583987\n",
      "  (0, 3861)\t0.23436398000967731\n",
      "  (0, 4027)\t0.19655659663513286\n",
      "  (0, 2023)\t0.3133027483289482\n",
      "  (0, 5691)\t0.1478328149366558\n",
      "  (0, 5433)\t0.21336485559773063\n",
      "  (0, 4174)\t0.18489753495978478\n",
      "  (0, 618)\t0.2985170498722688\n",
      "  (0, 5771)\t0.1137690531662858\n",
      "  (0, 5185)\t0.2876409315293689\n",
      "  (0, 4080)\t0.16409775986675704\n",
      "  (0, 1074)\t0.22710102287538758\n",
      "  (0, 4715)\t0.13709997670868124\n",
      "  (1, 5946)\t0.27013857869066665\n",
      "  (1, 5802)\t0.4048104966318018\n",
      "  (1, 2536)\t0.27872791703003025\n",
      "  (1, 3245)\t0.48799632295257916\n",
      "  (1, 2437)\t0.5022002917280878\n",
      "  (1, 3622)\t0.4417225533757436\n",
      "  :\t:\n",
      "  (4929, 1134)\t0.3175646443398614\n",
      "  (4929, 1903)\t0.3076184565497891\n",
      "  (4929, 1546)\t0.3076184565497891\n",
      "  (4929, 645)\t0.27919902099629856\n",
      "  (4929, 6172)\t0.3513534996402211\n",
      "  (4929, 4241)\t0.15077846403982767\n",
      "  (4929, 5693)\t0.11176515204920058\n",
      "  (4929, 4076)\t0.1451335090195885\n",
      "  (4930, 6420)\t0.4347631356565823\n",
      "  (4930, 3246)\t0.39093566195694407\n",
      "  (4930, 4218)\t0.3550818126245657\n",
      "  (4930, 5113)\t0.41988244369015143\n",
      "  (4930, 5879)\t0.21475901536265196\n",
      "  (4930, 6035)\t0.27728164856029214\n",
      "  (4930, 719)\t0.30622843029349955\n",
      "  (4930, 4151)\t0.165390670831297\n",
      "  (4930, 2962)\t0.27257602683843535\n",
      "  (4930, 3443)\t0.131069560917938\n",
      "  (4930, 5771)\t0.14195975090227836\n",
      "  (4931, 5717)\t0.5501772950293432\n",
      "  (4931, 2197)\t0.48607637689806604\n",
      "  (4931, 3313)\t0.5087067839586393\n",
      "  (4931, 1503)\t0.3795434733093844\n",
      "  (4931, 5693)\t0.1558948643214452\n",
      "  (4931, 4127)\t0.18410771563611708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Take a look at the head of the desc field\n",
    "print(ufo['desc'].head(),\"\\n\\n\")\n",
    "\n",
    "# Instantiate the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "ufo.dropna(subset=['desc'], inplace=True)# If we didn't do this, it would give \n",
    "                                         # ValueError: np.nan is an invalid document, expected byte or unicode string. \n",
    "                                         # ---> desc_tfidf = vec.fit_transform(ufo['desc'])\n",
    "\n",
    "# Fit and transform desc using vec\n",
    "desc_tfidf = vec.fit_transform(ufo['desc'])\n",
    "\n",
    "# Look at the number of columns and rows\n",
    "print(desc_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You'll notice that the text vector has a large number of columns. We'll work on selecting the features we want to use for modeling in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the ideal dataset\n",
    "Now to get rid of some of the unnecessary features in the ufo dataset. Because the country column has been encoded as country_enc, you can select it and drop the other columns related to location: city, country, lat, long, and state.\n",
    "\n",
    "You've engineered the month and year columns, so you no longer need the date or recorded columns. You also standardized the seconds column as seconds_log, so you can drop seconds and minutes.\n",
    "\n",
    "You vectorized desc, so it can be removed. For now you'll keep type.\n",
    "\n",
    "You can also get rid of the length_of_time column, which is unnecessary after extracting minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {1048: 'web',\n",
    " 278: 'designer',\n",
    " 1017: 'urban',\n",
    " 38: 'adventures',\n",
    " 490: 'ice',\n",
    " 890: 'skating',\n",
    " 90: 'at',\n",
    " 559: 'lasker',\n",
    " 832: 'rink',\n",
    " 368: 'fight',\n",
    " 423: 'global',\n",
    " 487: 'hunger',\n",
    " 68: 'and',\n",
    " 944: 'support',\n",
    " 1061: 'women',\n",
    " 356: 'farmers',\n",
    " 535: 'join',\n",
    " 969: 'the',\n",
    " 708: 'oxfam',\n",
    " 27: 'action',\n",
    " 240: 'corps',\n",
    " 498: 'in',\n",
    " 680: 'nyc',\n",
    " 922: 'stop',\n",
    " 947: 'swap',\n",
    " 790: 'queens',\n",
    " 911: 'staff',\n",
    " 281: 'development',\n",
    " 992: 'trainer',\n",
    " 200: 'claro',\n",
    " 145: 'brooklyn',\n",
    " 1037: 'volunteer',\n",
    " 93: 'attorney',\n",
    " 221: 'community',\n",
    " 455: 'health',\n",
    " 43: 'advocates',\n",
    " 942: 'supervise',\n",
    " 189: 'children',\n",
    " 466: 'highland',\n",
    " 717: 'park',\n",
    " 409: 'garden',\n",
    " 1071: 'worldofmoney',\n",
    " 696: 'org',\n",
    " 1085: 'youth',\n",
    " 60: 'amazing',\n",
    " 791: 'race',\n",
    " 789: 'qualified',\n",
    " 133: 'board',\n",
    " 620: 'member',\n",
    " 860: 'seats',\n",
    " 98: 'available',\n",
    " 1083: 'young',\n",
    " 33: 'adult',\n",
    " 1006: 'tutor',\n",
    " 1016: 'updated',\n",
    " 11: '30',\n",
    " 0: '11',\n",
    " 513: 'insurance',\n",
    " 199: 'claims',\n",
    " 600: 'manager',\n",
    " 979: 'timebanksnyc',\n",
    " 432: 'great',\n",
    " 340: 'exchange',\n",
    " 205: 'clean',\n",
    " 1015: 'up',\n",
    " 81: 'asbury',\n",
    " 171: 'cementary',\n",
    " 918: 'staten',\n",
    " 524: 'island',\n",
    " 869: 'senior',\n",
    " 194: 'citizen',\n",
    " 392: 'friendly',\n",
    " 1033: 'visitor',\n",
    " 881: 'shop',\n",
    " 1000: 'tree',\n",
    " 161: 'care',\n",
    " 1068: 'workshop',\n",
    " 4: '20',\n",
    " 646: 'movie',\n",
    " 856: 'screener',\n",
    " 380: 'for',\n",
    " 870: 'seniors',\n",
    " 355: 'farm',\n",
    " 430: 'graphic',\n",
    " 691: 'open',\n",
    " 480: 'house',\n",
    " 416: 'get',\n",
    " 984: 'tools',\n",
    " 980: 'to',\n",
    " 806: 'recycling',\n",
    " 1039: 'volunteers',\n",
    " 660: 'needed',\n",
    " 353: 'family',\n",
    " 336: 'event',\n",
    " 207: 'clerical',\n",
    " 158: 'cancer',\n",
    " 1041: 'walk',\n",
    " 120: 'befitnyc',\n",
    " 739: 'physical',\n",
    " 30: 'activity',\n",
    " 700: 'organizers',\n",
    " 269: 'decision',\n",
    " 266: 'day',\n",
    " 5: '2011',\n",
    " 661: 'needs',\n",
    " 1084: 'your',\n",
    " 459: 'help',\n",
    " 405: 'gain',\n",
    " 1021: 'valuable',\n",
    " 245: 'counseling',\n",
    " 344: 'experience',\n",
    " 687: 'on',\n",
    " 845: 'samaritans',\n",
    " 9: '24',\n",
    " 479: 'hour',\n",
    " 255: 'crisis',\n",
    " 478: 'hotline',\n",
    " 457: 'heart',\n",
    " 407: 'gallery',\n",
    " 703: 'our',\n",
    " 503: 'info',\n",
    " 949: 'table',\n",
    " 373: 'finding',\n",
    " 471: 'homes',\n",
    " 542: 'kids',\n",
    " 1077: 'yiddish',\n",
    " 903: 'speaking',\n",
    " 472: 'homework',\n",
    " 460: 'helper',\n",
    " 892: 'skilled',\n",
    " 800: 'rebuilding',\n",
    " 982: 'together',\n",
    " 468: 'home',\n",
    " 818: 'repairs',\n",
    " 438: 'greenteam',\n",
    " 40: 'advetures',\n",
    " 940: 'summer',\n",
    " 931: 'streets',\n",
    " 1005: 'tuesday',\n",
    " 335: 'evenings',\n",
    " 1060: 'with',\n",
    " 612: 'masa',\n",
    " 594: 'lunch',\n",
    " 770: 'program',\n",
    " 1018: 'us',\n",
    " 706: 'outreach',\n",
    " 618: 'meals',\n",
    " 760: 'preparedness',\n",
    " 222: 'compost',\n",
    " 773: 'project',\n",
    " 613: 'master',\n",
    " 223: 'composter',\n",
    " 178: 'certificate',\n",
    " 249: 'course',\n",
    " 318: 'emblemhealth',\n",
    " 144: 'bronx',\n",
    " 683: 'of',\n",
    " 873: 'service',\n",
    " 531: 'jcc',\n",
    " 601: 'manhattan',\n",
    " 418: 'girl',\n",
    " 855: 'scout',\n",
    " 872: 'series',\n",
    " 296: 'dorot',\n",
    " 838: 'rosh',\n",
    " 452: 'hashanah',\n",
    " 709: 'package',\n",
    " 274: 'delivery',\n",
    " 713: 'painting',\n",
    " 511: 'instructor',\n",
    " 530: 'jasa',\n",
    " 464: 'hes',\n",
    " 172: 'center',\n",
    " 12: '3rd',\n",
    " 70: 'annual',\n",
    " 377: 'flyny',\n",
    " 548: 'kite',\n",
    " 366: 'festival',\n",
    " 983: 'tomorrow',\n",
    " 151: 'business',\n",
    " 566: 'leaders',\n",
    " 955: 'teach',\n",
    " 110: 'basics',\n",
    " 465: 'high',\n",
    " 852: 'schoolers',\n",
    " 410: 'gardening',\n",
    " 397: 'ft',\n",
    " 1004: 'tryon',\n",
    " 910: 'st',\n",
    " 610: 'martin',\n",
    " 748: 'poetry',\n",
    " 668: 'new',\n",
    " 1079: 'york',\n",
    " 216: 'college',\n",
    " 424: 'goal',\n",
    " 941: 'sunday',\n",
    " 361: 'february',\n",
    " 6: '2012',\n",
    " 262: 'dance',\n",
    " 8: '22nd',\n",
    " 560: 'latino',\n",
    " 604: 'march',\n",
    " 2: '17',\n",
    " 1013: 'university',\n",
    " 848: 'saturday',\n",
    " 1008: 'tutors',\n",
    " 744: 'planet',\n",
    " 485: 'human',\n",
    " 602: 'mapping',\n",
    " 420: 'give',\n",
    " 1050: 'week',\n",
    " 186: 'child',\n",
    " 569: 'learn',\n",
    " 796: 'read',\n",
    " 926: 'storytelling',\n",
    " 243: 'costume',\n",
    " 597: 'making',\n",
    " 912: 'stage',\n",
    " 277: 'design',\n",
    " 319: 'emergency',\n",
    " 351: 'fair',\n",
    " 17: '9th',\n",
    " 1053: 'west',\n",
    " 887: 'side',\n",
    " 248: 'county',\n",
    " 676: 'nutrition',\n",
    " 314: 'educator',\n",
    " 879: 'shape',\n",
    " 306: 'east',\n",
    " 13: '54st',\n",
    " 801: 'rec',\n",
    " 1046: 'water',\n",
    " 45: 'aerobics',\n",
    " 83: 'asser',\n",
    " 573: 'levy',\n",
    " 712: 'paint',\n",
    " 57: 'alongside',\n",
    " 783: 'publicolor',\n",
    " 936: 'students',\n",
    " 536: 'jumpstart',\n",
    " 797: 'readers',\n",
    " 564: 'lead',\n",
    " 252: 'crafts',\n",
    " 408: 'games',\n",
    " 348: 'face',\n",
    " 751: 'popcorn',\n",
    " 527: 'jackie',\n",
    " 835: 'robinson',\n",
    " 716: 'parent',\n",
    " 375: 'fitness',\n",
    " 916: 'starrett',\n",
    " 197: 'city',\n",
    " 585: 'line',\n",
    " 263: 'dancer',\n",
    " 615: 'math',\n",
    " 587: 'literacy',\n",
    " 114: 'be',\n",
    " 209: 'climb',\n",
    " 985: 'top',\n",
    " 608: 'marketing',\n",
    " 86: 'assistant',\n",
    " 313: 'education',\n",
    " 673: 'nonprofit',\n",
    " 867: 'seeks',\n",
    " 805: 'recruitment',\n",
    " 626: 'mentors',\n",
    " 810: 'register',\n",
    " 92: 'attend',\n",
    " 142: 'breakfast',\n",
    " 701: 'orientation',\n",
    " 529: 'january',\n",
    " 272: 'deliver',\n",
    " 1058: 'winter',\n",
    " 1031: 'visit',\n",
    " 65: 'an',\n",
    " 525: 'isolated',\n",
    " 342: 'exercise',\n",
    " 213: 'coach',\n",
    " 670: 'night',\n",
    " 115: 'beach',\n",
    " 180: 'change',\n",
    " 77: 'art',\n",
    " 772: 'programs',\n",
    " 229: 'consumer',\n",
    " 779: 'protection',\n",
    " 562: 'law',\n",
    " 589: 'liver',\n",
    " 579: 'life',\n",
    " 565: 'leader',\n",
    " 901: 'soup',\n",
    " 547: 'kitchen',\n",
    " 307: 'eastern',\n",
    " 534: 'john',\n",
    " 650: 'muir',\n",
    " 930: 'street',\n",
    " 1024: 'vendor',\n",
    " 641: 'monthly',\n",
    " 959: 'team',\n",
    " 367: 'fiesta',\n",
    " 977: 'throgs',\n",
    " 658: 'neck',\n",
    " 224: 'computer',\n",
    " 956: 'teacher',\n",
    " 567: 'leadership',\n",
    " 244: 'council',\n",
    " 693: 'opportunity',\n",
    " 231: 'conversation',\n",
    " 461: 'helpers',\n",
    " 427: 'grades',\n",
    " 714: 'pantry',\n",
    " 288: 'distribution',\n",
    " 305: 'earth',\n",
    " 960: 'tech',\n",
    " 1049: 'website',\n",
    " 692: 'opportunities',\n",
    " 175: 'cents',\n",
    " 19: 'ability',\n",
    " 203: 'classroom',\n",
    " 877: 'set',\n",
    " 146: 'brush',\n",
    " 545: 'kindness',\n",
    " 999: 'transportation',\n",
    " 58: 'alternatives',\n",
    " 129: 'bike',\n",
    " 1020: 'valet',\n",
    " 1026: 'video',\n",
    " 311: 'editing',\n",
    " 767: 'professionals',\n",
    " 921: 'stipend',\n",
    " 49: 'after',\n",
    " 851: 'school',\n",
    " 624: 'mentor',\n",
    " 666: 'networking',\n",
    " 138: 'bowling',\n",
    " 398: 'fun',\n",
    " 449: 'harlem',\n",
    " 555: 'lanes',\n",
    " 866: 'seeking',\n",
    " 1078: 'yoga',\n",
    " 902: 'spanish',\n",
    " 695: 'or',\n",
    " 389: 'french',\n",
    " 362: 'feed',\n",
    " 488: 'hungry',\n",
    " 1080: 'yorkers',\n",
    " 14: '55',\n",
    " 690: 'only',\n",
    " 735: 'phone',\n",
    " 106: 'bank',\n",
    " 819: 'representative',\n",
    " 795: 'reach',\n",
    " 704: 'out',\n",
    " 643: 'morris',\n",
    " 458: 'heights',\n",
    " 904: 'special',\n",
    " 155: 'camp',\n",
    " 946: 'susan',\n",
    " 551: 'komen',\n",
    " 259: 'cure',\n",
    " 433: 'greater',\n",
    " 47: 'affiliate',\n",
    " 303: 'dumbo',\n",
    " 79: 'arts',\n",
    " 698: 'organizational',\n",
    " 148: 'budget',\n",
    " 639: 'money',\n",
    " 596: 'makes',\n",
    " 871: 'sense',\n",
    " 994: 'training',\n",
    " 889: 'site',\n",
    " 1027: 'videographer',\n",
    " 376: 'fly',\n",
    " 152: 'by',\n",
    " 970: 'theater',\n",
    " 429: 'grant',\n",
    " 1074: 'writer',\n",
    " 745: 'planning',\n",
    " 778: 'proposal',\n",
    " 759: 'preparation',\n",
    " 399: 'fund',\n",
    " 793: 'raising',\n",
    " 450: 'harm',\n",
    " 808: 'reduction',\n",
    " 35: 'adv',\n",
    " 515: 'intern',\n",
    " 875: 'serving',\n",
    " 575: 'lgbt',\n",
    " 34: 'adults',\n",
    " 482: 'how',\n",
    " 830: 'ride',\n",
    " 130: 'bikes',\n",
    " 821: 'research',\n",
    " 401: 'fundraising',\n",
    " 280: 'developement',\n",
    " 233: 'cook',\n",
    " 840: 'row',\n",
    " 50: 'afterschool',\n",
    " 630: 'middle',\n",
    " 885: 'shower',\n",
    " 400: 'fundraisers',\n",
    " 526: 'it',\n",
    " 519: 'interpreters',\n",
    " 563: 'lawyers',\n",
    " 446: 'haitian',\n",
    " 18: 'abe',\n",
    " 757: 'pre',\n",
    " 412: 'ged',\n",
    " 640: 'monitor',\n",
    " 89: 'astoria',\n",
    " 634: 'million',\n",
    " 1001: 'trees',\n",
    " 421: 'giveaway',\n",
    " 290: 'do',\n",
    " 1081: 'you',\n",
    " 1044: 'want',\n",
    " 595: 'make',\n",
    " 283: 'difference',\n",
    " 204: 'classwish',\n",
    " 896: 'snow',\n",
    " 883: 'shoveling',\n",
    " 196: 'citizenship',\n",
    " 761: 'press',\n",
    " 586: 'list',\n",
    " 781: 'public',\n",
    " 813: 'relations',\n",
    " 743: 'plan',\n",
    " 829: 'review',\n",
    " 394: 'friendship',\n",
    " 753: 'positive',\n",
    " 121: 'beginnings',\n",
    " 546: 'kit',\n",
    " 611: 'mary',\n",
    " 803: 'recreation',\n",
    " 291: 'does',\n",
    " 697: 'organization',\n",
    " 659: 'need',\n",
    " 858: 'search',\n",
    " 928: 'strategy',\n",
    " 332: 'esl',\n",
    " 46: 'affected',\n",
    " 924: 'storm',\n",
    " 995: 'transform',\n",
    " 590: 'lives',\n",
    " 933: 'strengthen',\n",
    " 220: 'communities',\n",
    " 119: 'become',\n",
    " 302: 'driver',\n",
    " 1025: 'veterans',\n",
    " 191: 'chinese',\n",
    " 997: 'translator',\n",
    " 512: 'instructors',\n",
    " 653: 'museum',\n",
    " 621: 'membership',\n",
    " 275: 'department',\n",
    " 284: 'director',\n",
    " 117: 'beautify',\n",
    " 996: 'transitional',\n",
    " 822: 'residence',\n",
    " 470: 'homeless',\n",
    " 623: 'men',\n",
    " 953: 'tank',\n",
    " 517: 'internship',\n",
    " 774: 'projects',\n",
    " 841: 'run',\n",
    " 1056: 'wild',\n",
    " 139: 'boys',\n",
    " 475: 'hope',\n",
    " 419: 'girls',\n",
    " 219: 'communications',\n",
    " 792: 'raise',\n",
    " 100: 'awareness',\n",
    " 31: 'administrative',\n",
    " 56: 'alliance',\n",
    " 811: 'registrar',\n",
    " 647: 'ms',\n",
    " 1062: 'word',\n",
    " 162: 'career',\n",
    " 246: 'counselor',\n",
    " 722: 'passover',\n",
    " 304: 'early',\n",
    " 188: 'childhood',\n",
    " 149: 'build',\n",
    " 747: 'plastic',\n",
    " 137: 'bottle',\n",
    " 857: 'sculpture',\n",
    " 763: 'pride',\n",
    " 523: 'is',\n",
    " 538: 'just',\n",
    " 76: 'around',\n",
    " 238: 'corner',\n",
    " 520: 'involved',\n",
    " 675: 'now',\n",
    " 390: 'fresh',\n",
    " 53: 'air',\n",
    " 957: 'teachers',\n",
    " 372: 'find',\n",
    " 729: 'perfect',\n",
    " 533: 'job',\n",
    " 684: 'office',\n",
    " 1075: 'writing',\n",
    " 264: 'data',\n",
    " 326: 'entry',\n",
    " 29: 'activism',\n",
    " 738: 'photography',\n",
    " 843: 'salesforce',\n",
    " 265: 'database',\n",
    " 261: 'customization',\n",
    " 736: 'photo',\n",
    " 333: 'essay',\n",
    " 572: 'legal',\n",
    " 42: 'advisor',\n",
    " 467: 'hike',\n",
    " 974: 'thon',\n",
    " 236: 'coordinator',\n",
    " 558: 'laser',\n",
    " 950: 'tag',\n",
    " 298: 'dowling',\n",
    " 3: '175th',\n",
    " 505: 'information',\n",
    " 962: 'technology',\n",
    " 352: 'fall',\n",
    " 382: 'forest',\n",
    " 826: 'restoration',\n",
    " 541: 'kickoff',\n",
    " 1002: 'trevor',\n",
    " 582: 'lifeline',\n",
    " 247: 'counselors',\n",
    " 973: 'thomas',\n",
    " 532: 'jefferson',\n",
    " 614: 'materials',\n",
    " 1076: 'year',\n",
    " 386: 'founder',\n",
    " 341: 'executive',\n",
    " 453: 'haunted',\n",
    " 557: 'lantern',\n",
    " 989: 'tours',\n",
    " 383: 'fort',\n",
    " 986: 'totten',\n",
    " 657: 'national',\n",
    " 878: 'sexual',\n",
    " 82: 'assault',\n",
    " 689: 'online',\n",
    " 993: 'trainers',\n",
    " 48: 'african',\n",
    " 63: 'american',\n",
    " 210: 'clothing',\n",
    " 301: 'drive',\n",
    " 828: 'returning',\n",
    " 865: 'seeds',\n",
    " 939: 'success',\n",
    " 746: 'plant',\n",
    " 981: 'today',\n",
    " 443: 'growth',\n",
    " 1009: 'udec',\n",
    " 328: 'enviromedia',\n",
    " 636: 'mobile',\n",
    " 606: 'maritime',\n",
    " 102: 'bacchanal',\n",
    " 742: 'pirates',\n",
    " 365: 'fest',\n",
    " 492: 'ikea',\n",
    " 329: 'erie',\n",
    " 111: 'basin',\n",
    " 282: 'diabetes',\n",
    " 88: 'association',\n",
    " 364: 'feria',\n",
    " 267: 'de',\n",
    " 844: 'salud',\n",
    " 664: 'nepali',\n",
    " 105: 'bangla',\n",
    " 784: 'punjabi',\n",
    " 998: 'translators',\n",
    " 674: 'not',\n",
    " 769: 'profit',\n",
    " 741: 'pioneer',\n",
    " 159: 'capoeira',\n",
    " 1023: 'various',\n",
    " 752: 'positions',\n",
    " 287: 'dispatcher',\n",
    " 991: 'trainee',\n",
    " 506: 'ing',\n",
    " 603: 'marathon',\n",
    " 388: 'free',\n",
    " 593: 'love',\n",
    " 135: 'books',\n",
    " 268: 'dear',\n",
    " 96: 'authors',\n",
    " 52: 'aide',\n",
    " 850: 'scheuer',\n",
    " 627: 'merchandise',\n",
    " 293: 'donate',\n",
    " 943: 'supplies',\n",
    " 360: 'feast',\n",
    " 406: 'gala',\n",
    " 112: 'battery',\n",
    " 833: 'rise',\n",
    " 919: 'stay',\n",
    " 787: 'put',\n",
    " 820: 'rescue',\n",
    " 897: 'soccer',\n",
    " 402: 'futsal',\n",
    " 730: 'performing',\n",
    " 36: 'advanced',\n",
    " 202: 'classes',\n",
    " 1070: 'world',\n",
    " 854: 'science',\n",
    " 1054: 'western',\n",
    " 64: 'americorps',\n",
    " 25: 'aces',\n",
    " 310: 'economic',\n",
    " 864: 'security',\n",
    " 507: 'initiative',\n",
    " 331: 'esi',\n",
    " 633: 'mill',\n",
    " 173: 'centers',\n",
    " 631: 'midtown',\n",
    " 1088: 'zumba',\n",
    " 1030: 'vision',\n",
    " 635: 'mission',\n",
    " 66: 'analysis',\n",
    " 552: 'lab',\n",
    " 958: 'teaching',\n",
    " 84: 'assist',\n",
    " 827: 'resume',\n",
    " 150: 'building',\n",
    " 899: 'society',\n",
    " 214: 'coaches',\n",
    " 1040: 'vs',\n",
    " 218: 'committee',\n",
    " 842: 'russian',\n",
    " 385: 'foster',\n",
    " 170: 'celebration',\n",
    " 616: 'may',\n",
    " 7: '21th',\n",
    " 688: 'one',\n",
    " 711: 'pager',\n",
    " 294: 'donation',\n",
    " 489: 'hurricane',\n",
    " 521: 'irene',\n",
    " 354: 'far',\n",
    " 836: 'rockaway',\n",
    " 325: 'enjoy',\n",
    " 1066: 'working',\n",
    " 686: 'olympics',\n",
    " 988: 'tournament',\n",
    " 798: 'reading',\n",
    " 719: 'partners',\n",
    " 234: 'cooper',\n",
    " 909: 'square',\n",
    " 975: 'thrift',\n",
    " 908: 'spring',\n",
    " 166: 'case',\n",
    " 599: 'management',\n",
    " 404: 'fvcp',\n",
    " 990: 'trail',\n",
    " 254: 'crew',\n",
    " 447: 'halloween',\n",
    " 165: 'carnival',\n",
    " 1042: 'walkathon',\n",
    " 359: 'feasibility',\n",
    " 67: 'analyst',\n",
    " 749: 'police',\n",
    " 868: 'seminar',\n",
    " 1064: 'work',\n",
    " 1035: 'visually',\n",
    " 496: 'impaired',\n",
    " 964: 'teens',\n",
    " 972: 'this',\n",
    " 322: 'energy',\n",
    " 315: 'efficiency',\n",
    " 321: 'end',\n",
    " 859: 'season',\n",
    " 156: 'campaign',\n",
    " 123: 'benefits',\n",
    " 802: 'reception',\n",
    " 300: 'drill',\n",
    " 237: 'copywriting',\n",
    " 235: 'coord',\n",
    " 454: 'have',\n",
    " 725: 'penchant',\n",
    " 55: 'all',\n",
    " 971: 'things',\n",
    " 1028: 'vintage',\n",
    " 976: 'thriftshop',\n",
    " 718: 'partner',\n",
    " 726: 'pencil',\n",
    " 720: 'partnership',\n",
    " 710: 'packing',\n",
    " 16: '8th',\n",
    " 907: 'sports',\n",
    " 346: 'expo',\n",
    " 164: 'cares',\n",
    " 184: 'cheerleaders',\n",
    " 1045: 'wanted',\n",
    " 445: 'habitat',\n",
    " 371: 'finance',\n",
    " 215: 'coffee',\n",
    " 324: 'english',\n",
    " 755: 'practice',\n",
    " 570: 'learners',\n",
    " 456: 'healthy',\n",
    " 28: 'active',\n",
    " 978: 'time',\n",
    " 122: 'benefit',\n",
    " 73: 'april',\n",
    " 357: 'fashion',\n",
    " 929: 'strawberry',\n",
    " 87: 'assistants',\n",
    " 174: 'central',\n",
    " 1087: 'zoo',\n",
    " 1: '125th',\n",
    " 127: 'bideawee',\n",
    " 440: 'greeters',\n",
    " 592: 'looking',\n",
    " 799: 'real',\n",
    " 495: 'impact',\n",
    " 504: 'inform',\n",
    " 728: 'people',\n",
    " 756: 'practices',\n",
    " 580: 'lifebeat',\n",
    " 413: 'general',\n",
    " 932: 'streetsquash',\n",
    " 286: 'discovery',\n",
    " 874: 'services',\n",
    " 663: 'neighborhood',\n",
    " 768: 'profiles',\n",
    " 951: 'take',\n",
    " 915: 'stand',\n",
    " 51: 'against',\n",
    " 1029: 'violence',\n",
    " 345: 'expert',\n",
    " 41: 'advice',\n",
    " 537: 'june',\n",
    " 849: 'schedule',\n",
    " 258: 'crowdfunding',\n",
    " 727: 'penny',\n",
    " 451: 'harvest',\n",
    " 434: 'green',\n",
    " 185: 'chefs',\n",
    " 677: 'nutritionists',\n",
    " 379: 'foodies',\n",
    " 625: 'mentoring',\n",
    " 136: 'boom',\n",
    " 669: 'newsletter',\n",
    " 217: 'come',\n",
    " 934: 'strides',\n",
    " 1043: 'walks',\n",
    " 187: 'childcare',\n",
    " 898: 'social',\n",
    " 619: 'media',\n",
    " 422: 'giving',\n",
    " 157: 'can',\n",
    " 61: 'ambassador',\n",
    " 10: '2nd',\n",
    " 967: 'thanksgiving',\n",
    " 363: 'feeding',\n",
    " 662: 'needy',\n",
    " 782: 'publicity',\n",
    " 723: 'patient',\n",
    " 163: 'caregiver',\n",
    " 1032: 'visiting',\n",
    " 469: 'homebound',\n",
    " 358: 'fc',\n",
    " 679: 'nyawc',\n",
    " 384: 'forum',\n",
    " 21: 'about',\n",
    " 1038: 'volunteering',\n",
    " 809: 'refreshments',\n",
    " 847: 'sara',\n",
    " 837: 'roosevelt',\n",
    " 206: 'cleanup',\n",
    " 116: 'beautification',\n",
    " 337: 'events',\n",
    " 69: 'animal',\n",
    " 484: 'hudson',\n",
    " 834: 'river',\n",
    " 605: 'mariners',\n",
    " 825: 'response',\n",
    " 343: 'exhibit',\n",
    " 20: 'aboard',\n",
    " 584: 'lilac',\n",
    " 208: 'client',\n",
    " 1052: 'welcome',\n",
    " 279: 'desk',\n",
    " 685: 'older',\n",
    " 574: 'lexington',\n",
    " 251: 'craft',\n",
    " 750: 'poll',\n",
    " 1065: 'workers',\n",
    " 518: 'interperters',\n",
    " 24: 'accounting',\n",
    " 85: 'assistance',\n",
    " 477: 'hosting',\n",
    " 776: 'promotion',\n",
    " 1011: 'unicef',\n",
    " 954: 'tap',\n",
    " 814: 'release',\n",
    " 270: 'dedication',\n",
    " 771: 'programming',\n",
    " 500: 'incarnation',\n",
    " 295: 'donor',\n",
    " 544: 'kieran',\n",
    " 906: 'sponsorship',\n",
    " 1069: 'workshops',\n",
    " 118: 'because',\n",
    " 338: 'every',\n",
    " 276: 'deserves',\n",
    " 179: 'chance',\n",
    " 740: 'pin',\n",
    " 273: 'delivered',\n",
    " 886: 'shred',\n",
    " 15: '5th',\n",
    " 99: 'avenue',\n",
    " 169: 'cdsc',\n",
    " 917: 'starving',\n",
    " 78: 'artist',\n",
    " 884: 'show',\n",
    " 948: 'system',\n",
    " 396: 'front',\n",
    " 880: 'share',\n",
    " 553: 'lanch',\n",
    " 935: 'student',\n",
    " 463: 'hemophilia',\n",
    " 577: 'liason',\n",
    " 629: 'methodist',\n",
    " 476: 'hospital',\n",
    " 113: 'bay',\n",
    " 831: 'ridge',\n",
    " 124: 'benonhurst',\n",
    " 75: 'area',\n",
    " 900: 'sought',\n",
    " 97: 'autistic',\n",
    " 297: 'douglaston',\n",
    " 788: 'qns',\n",
    " 812: 'registration',\n",
    " 32: 'administrator',\n",
    " 153: 'call',\n",
    " 426: 'governor',\n",
    " 804: 'recruiter',\n",
    " 786: 'purim',\n",
    " 327: 'envelope',\n",
    " 938: 'stuffing',\n",
    " 528: 'jam',\n",
    " 462: 'helpline',\n",
    " 923: 'store',\n",
    " 374: 'first',\n",
    " 415: 'generation',\n",
    " 1022: 'van',\n",
    " 241: 'cortlandt',\n",
    " 816: 'remembrance',\n",
    " 945: 'survey',\n",
    " 823: 'resonations',\n",
    " 143: 'breast',\n",
    " 323: 'engine',\n",
    " 694: 'optimization',\n",
    " 622: 'memorial',\n",
    " 894: 'sloan',\n",
    " 540: 'kettering',\n",
    " 435: 'greenhouse',\n",
    " 436: 'greening',\n",
    " 227: 'concert',\n",
    " 334: 'evacuation',\n",
    " 824: 'resources',\n",
    " 417: 'gift',\n",
    " 126: 'bicycling',\n",
    " 656: 'my',\n",
    " 393: 'friends',\n",
    " 473: 'honor',\n",
    " 1051: 'weekend',\n",
    " 731: 'person',\n",
    " 651: 'mural',\n",
    " 312: 'editor',\n",
    " 732: 'personal',\n",
    " 882: 'shopper',\n",
    " 764: 'pro',\n",
    " 134: 'bono',\n",
    " 253: 'create',\n",
    " 160: 'cards',\n",
    " 920: 'step',\n",
    " 672: 'non',\n",
    " 780: 'provider',\n",
    " 516: 'interns',\n",
    " 645: 'motion',\n",
    " 431: 'graphics',\n",
    " 125: 'best',\n",
    " 147: 'buddies',\n",
    " 502: 'inern',\n",
    " 103: 'back',\n",
    " 588: 'little',\n",
    " 242: 'cosmetologist',\n",
    " 107: 'barber',\n",
    " 1036: 'vocational',\n",
    " 72: 'apartment',\n",
    " 439: 'greeter',\n",
    " 766: 'professional',\n",
    " 1019: 'use',\n",
    " 893: 'skills',\n",
    " 702: 'others',\n",
    " 369: 'figure',\n",
    " 257: 'croton',\n",
    " 190: 'chinatown',\n",
    " 193: 'ci',\n",
    " 758: 'prep',\n",
    " 239: 'corporate',\n",
    " 1063: 'wordpress',\n",
    " 132: 'blog',\n",
    " 510: 'instructer',\n",
    " 807: 'red',\n",
    " 474: 'hook',\n",
    " 289: 'divert',\n",
    " 966: 'textiles',\n",
    " 395: 'from',\n",
    " 554: 'landfill',\n",
    " 437: 'greenmarket',\n",
    " 965: 'textile',\n",
    " 154: 'calling',\n",
    " 195: 'citizens',\n",
    " 497: 'improve',\n",
    " 26: 'achievement',\n",
    " 721: 'passion',\n",
    " 481: 'housing',\n",
    " 1067: 'works',\n",
    " 499: 'inc',\n",
    " 441: 'group',\n",
    " 299: 'drama',\n",
    " 561: 'laundromats',\n",
    " 320: 'employment',\n",
    " 927: 'strategic',\n",
    " 667: 'never',\n",
    " 104: 'bad',\n",
    " 391: 'friend',\n",
    " 403: 'future',\n",
    " 201: 'class',\n",
    " 1059: 'wish',\n",
    " 387: 'fpcj',\n",
    " 1072: 'worship',\n",
    " 1010: 'undergraduate',\n",
    " 428: 'graduate',\n",
    " 228: 'conference',\n",
    " 1047: 'we',\n",
    " 775: 'promote',\n",
    " 550: 'knowledge',\n",
    " 715: 'parade',\n",
    " 74: 'archivist',\n",
    " 425: 'google',\n",
    " 44: 'adwords',\n",
    " 493: 'imentor',\n",
    " 642: 'more',\n",
    " 598: 'male',\n",
    " 632: 'miles',\n",
    " 637: 'moms',\n",
    " 183: 'charity',\n",
    " 176: 'century',\n",
    " 987: 'tour',\n",
    " 198: 'civil',\n",
    " 724: 'patrol',\n",
    " 62: 'america',\n",
    " 539: 'kept',\n",
    " 862: 'secret',\n",
    " 648: 'ms131',\n",
    " 549: 'knitter',\n",
    " 256: 'crochet',\n",
    " 131: 'blankets',\n",
    " 177: 'ceo',\n",
    " 591: 'logo',\n",
    " 1012: 'unique',\n",
    " 1057: 'will',\n",
    " 128: 'big',\n",
    " 37: 'adventure',\n",
    " 23: 'accountant',\n",
    " 876: 'session',\n",
    " 888: 'single',\n",
    " 644: 'mothers',\n",
    " 192: 'choice',\n",
    " 895: 'smc',\n",
    " 1055: 'wii',\n",
    " 705: 'outdoor',\n",
    " 671: 'nights',\n",
    " 607: 'market',\n",
    " 514: 'intake',\n",
    " 638: 'monday',\n",
    " 141: 'branding',\n",
    " 140: 'brand',\n",
    " 491: 'identity',\n",
    " 649: 'mt',\n",
    " 1086: 'zion',\n",
    " 543: 'kidz',\n",
    " 817: 'reorganize',\n",
    " 578: 'library',\n",
    " 378: 'food',\n",
    " 91: 'athletic',\n",
    " 568: 'league',\n",
    " 655: 'musician',\n",
    " 59: 'alzheimer',\n",
    " 654: 'music',\n",
    " 109: 'bash',\n",
    " 765: 'proctor',\n",
    " 952: 'taking',\n",
    " 339: 'exams',\n",
    " 777: 'promotional',\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "6204",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-edc472372c6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Let's also filter some words out of the text vector we created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mfiltered_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords_to_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-86-edc472372c6b>\u001b[0m in \u001b[0;36mwords_to_filter\u001b[1;34m(vocab, original_vocab, vector, top_n)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Call the return_weights function and extend filter_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mfilter_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-86-edc472372c6b>\u001b[0m in \u001b[0;36mreturn_weights\u001b[1;34m(vocab, original_vocab, vector, vector_index, top_n)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Transform that zipped dict into a series\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mzipped_series\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mzipped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvector_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Sort the series to pull out the top n weighted words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-86-edc472372c6b>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Transform that zipped dict into a series\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mzipped_series\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mzipped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvector_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Sort the series to pull out the top n weighted words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 6204"
     ]
    }
   ],
   "source": [
    "# Add in the rest of the arguments\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "\n",
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Call the return_weights function and extend filter_list\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "        \n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Make a list of features to drop   \n",
    "to_drop = [\"city\", \"country\", \"date\", \"desc\", \"lat\", \"length_of_time\", \"long\", \"minutes\", \"recorded\", \"seconds\", \"state\"]\n",
    "\n",
    "# Drop those features\n",
    "ufo_dropped = ufo.drop(to_drop, axis=1)\n",
    "\n",
    "# Let's also filter some words out of the text vector we created\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You're almost done. In the next exercises, you'll model the UFO data in a couple of different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7591240875912408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# X = ufo.loc[:,['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone', 'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash', 'formation', 'light', 'other', 'oval', 'rectangle',\n",
    "           #'sphere', 'teardrop', 'triangle', 'unknown', 'month', 'year']]\n",
    "    \n",
    "X = ufo.drop(['date', 'city', 'state', 'country', 'type', 'seconds', 'length_of_time',\n",
    "       'desc', 'recorded', 'lat', 'long', 'minutes', 'seconds_log',\n",
    "       'country_enc'], axis = 1)\n",
    "\n",
    "y = ufo.loc[:,[\"country_enc\"]]\n",
    "\n",
    "# Split the X and y sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 42)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit knn to the training sets\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of knn on the test sets\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome work! This model performs pretty well. It seems like you've made pretty good feature selection choices here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling the UFO dataset, part 2\n",
    "Finally, you'll build a model using the text vector we created, desc_tfidf, using the filtered_words list to create a filtered text vector. Let's see if you can predict the type of the sighting based on the text. You'll use a Naive Bayes model for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-3cc28c3f9b95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Use the list of filtered words we created to filter the text vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mfiltered_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdesc_tfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Split the X and y sets using train_test_split, setting stratify=y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered_words' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y \n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit nb to the training sets\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of nb on the test sets\n",
    "print(nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    0.17987152034261242"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats, you've completed the course! As you can see, this model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
